{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NML645P2MiVG",
        "outputId": "439ee8c7-14be-45a3-cd29-44382d0b5d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch datasets transformers peft accelerate bitsandbytes datasets -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rq37nQQkgNG",
        "outputId": "186a31e0-316d-4281-ae9e-b8594b68426c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/sft_marcus.jsonl\n"
          ]
        }
      ],
      "source": [
        "data_file = \"sft_marcus.jsonl\"\n",
        "FILE_PATH = f\"/content/drive/MyDrive/{data_file}\"\n",
        "print(FILE_PATH)\n",
        "REPO_NAME = \"marcus-tinyllama-finetuned-large\"\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zib0KHX8N3VF",
        "outputId": "a4064b59-0e79-4a08-cafc-fd7ddec058e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple Fine-tuning Script for Open Source LLM\n",
        "Uses TinyLlama-1.1B (smallest suitable model) with LoRA for efficient training\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. Load your JSONL dataset\n",
        "# ================================\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=FILE_PATH, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra7gRsR8H_eY",
        "outputId": "0cc7a9a1-c145-4ece-caf4-47a3622a718e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple Fine-tuning Script for Open Source LLM\n",
        "Uses TinyLlama-1.1B (smallest suitable model) with LoRA for efficient training\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. Load your JSONL dataset\n",
        "# ================================\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=FILE_PATH, split=\"train\")\n",
        "\n",
        "\n",
        "class SimpleFineTuner:\n",
        "    def __init__(self, model_name=BASE_MODEL):\n",
        "        self.model_name = model_name\n",
        "        self.hf_token = os.getenv(\"HF_TOKEN\")\n",
        "        self.hf_org = \"iwswordpress\"\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        print(f\"Loading model: {self.model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name, token=self.hf_token, trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Add pad token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        # Configure LoRA for efficient fine-tuning\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=8,  # Low rank\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        )\n",
        "\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "        print(\n",
        "            f\"Model loaded with LoRA. Trainable parameters: {self.model.num_parameters()}\"\n",
        "        )\n",
        "\n",
        "    def load_jsonl_data(self, file_path):\n",
        "        \"\"\"Load and process JSONL training data\"\"\"\n",
        "        file_path = FILE_PATH\n",
        "        data = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                item = json.loads(line.strip())\n",
        "                # Format as chat template\n",
        "                formatted_text = f\"<|user|>\\n{item['prompt']}<|end|>\\n<|assistant|>\\n{item['response']}<|end|>\"\n",
        "                data.append({\"text\": formatted_text})\n",
        "\n",
        "        print(f\"Loaded {len(data)} training examples\")\n",
        "\n",
        "        return Dataset.from_list(data)\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenize the dataset\"\"\"\n",
        "        tokenized = self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,  # Keep it small for efficiency\n",
        "            return_overflowing_tokens=False,\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    def train(self, jsonl_file=\"data_file\", output_dir=\"./marcus-tinyllama-finetuned\"):\n",
        "        \"\"\"Fine-tune the model\"\"\"\n",
        "\n",
        "        # Load and prepare dataset\n",
        "        dataset = self.load_jsonl_data(jsonl_file)\n",
        "        tokenized_dataset = dataset.map(\n",
        "            self.tokenize_function, batched=True, remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        # Split dataset (80% train, 20% eval)\n",
        "        train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "        train_dataset = train_test_split[\"train\"]\n",
        "        eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "        # Training arguments - optimized for small model and quick training\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=100,\n",
        "            max_steps=500,  # Keep training short\n",
        "            learning_rate=2e-4,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            logging_steps=50,\n",
        "            eval_steps=100,\n",
        "            save_steps=200,  # Must be multiple of eval_steps\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=None,  # Disable wandb/tensorboard\n",
        "            remove_unused_columns=False,\n",
        "        )\n",
        "\n",
        "        # Initialize trainer without data collator (let it use default)\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Training completed! Model saved to {output_dir}\")\n",
        "        return output_dir\n",
        "\n",
        "    def test_model(self, model_path, test_prompt=\"What school did you go to?\"):\n",
        "        \"\"\"Test the fine-tuned model\"\"\"\n",
        "        print(f\"\\nTesting model with prompt: '{test_prompt}'\")\n",
        "\n",
        "        # Load the fine-tuned model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Format input\n",
        "        formatted_input = f\"<|user|>\\n{test_prompt}<|end|>\\n<|assistant|>\\n\"\n",
        "        inputs = tokenizer(formatted_input, return_tensors=\"pt\")\n",
        "\n",
        "        # Move inputs to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode and print response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Model response:\\n{response}\")\n",
        "\n",
        "    def upload_to_hub(self, model_path, repo_name=REPO_NAME):\n",
        "        \"\"\"Upload the fine-tuned model to Hugging Face Hub\"\"\"\n",
        "        from huggingface_hub import HfApi\n",
        "\n",
        "        full_repo_name = f\"{self.hf_org}/{repo_name}\"\n",
        "\n",
        "        try:\n",
        "            api = HfApi(token=self.hf_token)\n",
        "\n",
        "            # Create repository\n",
        "            api.create_repo(repo_id=full_repo_name, exist_ok=True, private=False)\n",
        "\n",
        "            # Upload model files\n",
        "            api.upload_folder(\n",
        "                folder_path=model_path, repo_id=full_repo_name, repo_type=\"model\"\n",
        "            )\n",
        "\n",
        "            print(\n",
        "                f\"Model uploaded successfully to: https://huggingface.co/{full_repo_name}\"\n",
        "            )\n",
        "            return full_repo_name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading to hub: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPX6pg9qIFIr"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the fine-tuning process\"\"\"\n",
        "    print(f\"Using {BASE_MODEL} with LoRA for efficient training\\n\")\n",
        "\n",
        "    # Initialize fine-tuner\n",
        "    finetuner = SimpleFineTuner()\n",
        "\n",
        "    # Train the model\n",
        "    model_path = finetuner.train()\n",
        "\n",
        "    # Test the model\n",
        "    finetuner.test_model(model_path)\n",
        "\n",
        "    # Ask user if they want to upload to Hub\n",
        "    upload_choice = input(\n",
        "        \"\\nWould you like to upload the model to Hugging Face Hub? (y/n): \"\n",
        "    )\n",
        "    if upload_choice.lower() == \"y\":\n",
        "        repo_name = input(f\"Enter repository name (default:{REPO_NAME}: \").strip()\n",
        "        if not repo_name:\n",
        "            repo_name = REPO_NAME\n",
        "\n",
        "        finetuner.upload_to_hub(model_path, repo_name)\n",
        "\n",
        "    print(\"\\nFine-tuning process completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f703e1c172b948a7b6ef1bfc10e9c130",
            "7aeb728997d948b19eb6d364dff0587a",
            "fb8c46ba53404bb58b00857a935280de",
            "0966aec395c44fad9c78c60d1e270b03",
            "d18ba19bb71a46caa554a689b29ec3ba",
            "2b3852d7101b487394b37918980e1407",
            "319ef572919f4e018bad3a2fb47d7e43",
            "443a9963e17d4f14827a1d0bd1fb7a1e",
            "4cc0016ed1ed42348038b00be5e173e8",
            "2e507fd149394872959c1450d65628b8",
            "022782e53e8240589de3450b8d2facf5"
          ]
        },
        "id": "tQNBbWrYID49",
        "outputId": "2c8c6822-30a9-46a1-868d-4805a826afc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using meta-llama/Meta-Llama-3.1-8B with LoRA for efficient training\n",
            "\n",
            "Loading model: meta-llama/Meta-Llama-3.1-8B\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f703e1c172b948a7b6ef1bfc10e9c130",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded with LoRA. Trainable parameters: 8037076992\n",
            "Loaded 86 training examples\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "022782e53e8240589de3450b8d2facf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0966aec395c44fad9c78c60d1e270b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e507fd149394872959c1450d65628b8",
            "placeholder": "​",
            "style": "IPY_MODEL_022782e53e8240589de3450b8d2facf5",
            "value": " 4/4 [00:04&lt;00:00,  1.06s/it]"
          }
        },
        "2b3852d7101b487394b37918980e1407": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e507fd149394872959c1450d65628b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "319ef572919f4e018bad3a2fb47d7e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "443a9963e17d4f14827a1d0bd1fb7a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc0016ed1ed42348038b00be5e173e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7aeb728997d948b19eb6d364dff0587a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3852d7101b487394b37918980e1407",
            "placeholder": "​",
            "style": "IPY_MODEL_319ef572919f4e018bad3a2fb47d7e43",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d18ba19bb71a46caa554a689b29ec3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f703e1c172b948a7b6ef1bfc10e9c130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aeb728997d948b19eb6d364dff0587a",
              "IPY_MODEL_fb8c46ba53404bb58b00857a935280de",
              "IPY_MODEL_0966aec395c44fad9c78c60d1e270b03"
            ],
            "layout": "IPY_MODEL_d18ba19bb71a46caa554a689b29ec3ba"
          }
        },
        "fb8c46ba53404bb58b00857a935280de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_443a9963e17d4f14827a1d0bd1fb7a1e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cc0016ed1ed42348038b00be5e173e8",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
